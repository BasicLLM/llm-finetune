{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 使用 GRPO 训练模型\n",
    "\n",
    "## GRPO 原理\n",
    "\n",
    "GRPO（Group Relative Policy Optimization，群体相对策略优化）是一种由DeepSeek团队提出的强化学习算法，旨在优化大语言模型（LLM）的推理能力。与传统PPO算法依赖价值函数评估动作优劣不同，GRPO通过生成多个候选答案构成“群体”，利用群体内奖励的相对差异（如归一化奖励或排名优势）计算优势函数，从而避免维护额外的价值网络，显著降低显存占用和计算成本。其核心创新包括：\n",
    "\n",
    " 1. **分组采样机制**，同一问题生成多组输出以评估相对质量；\n",
    " 2. **群体竞争优化**，通过奖励均值和标准差计算相对优势，引导模型强化优质答案；\n",
    " 3. **KL散度约束**，直接控制策略更新幅度以提升训练稳定性。\n",
    "\n",
    "此外，GRPO兼容LoRA等高效微调技术，可将显存需求降低至7GB，使消费级GPU也能支持复杂推理模型的训练，推动了AI技术平民化。该算法已在数学推理、代码生成等任务中验证了性能优势，并被迁移至视觉语言模型领域，展现了广泛的应用潜力。其原理图如下所示：\n",
    "\n",
    "![GRPO Struct](img/GRPO-Struct.svg)\n",
    "\n",
    "具体公式与原理参考 Deepseek-R1 论文第 2.2 章：[Github - DeepSeek_R1.pdf](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)\n",
    "\n",
    "## GRPO 训练\n",
    "\n",
    "### 1. 引入必要的库"
   ],
   "id": "d27d5d5a8be84fc5"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. 加载并设计训练数据\n",
    "\n",
    "这里我们定义了一个标签 `<answer></answer>` 我们期望训练的模型输出可以用该标签包裹，这里就是将训练集设置为指定的格式："
   ],
   "id": "11710f8048cfede9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_path = \"./dataset/tldr\"\n",
    "\n",
    "answer_label = 'answer'\n",
    "SYSTEM_PROMPT = f'你的回答需要在<{answer_label}></{answer_label}>标签内。'\n",
    "XML_COT_FORMAT = f'<{answer_label}>' + '{answer}' + f'</{answer_label}>'\n",
    "\n",
    "# 数据处理，将 assistant 的输出处理为指定格式，即用 <answer>...</answer> 的格式\n",
    "def get_dataset() -> Dataset:\n",
    "\n",
    "    data = Dataset.load_from_disk(data_path)\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            # few shot, 因为0.5B模型太弱了\n",
    "            {'role': 'user', 'content': '数字10203040里面有几个0?'},\n",
    "            {'role': 'assistant', 'content': XML_COT_FORMAT.format(answer='4个0')},\n",
    "            {'role': 'user', 'content': x['prompt']}\n",
    "        ],\n",
    "        'answer': x['completion']\n",
    "    })\n",
    "    return data\n",
    "\n",
    "dataset = get_dataset()\n",
    "dataset = dataset.remove_columns(\"completion\")"
   ],
   "id": "46a8503dd3b02606",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**检查数据内容：**",
   "id": "e7c2618078ddc3f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(dataset[0])",
   "id": "f952bc4e6f2ac9b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. 设计奖励函数（核心）\n",
    "\n",
    "这里的奖励函数需要根据具体的需求进行设计，具体可以参考下面的奖励函数："
   ],
   "id": "9fe4c816cc1bddff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(completion):\n",
    "    \"\"\"\n",
    "    提取答案\n",
    "\n",
    "    :param completion: 补全的内容\n",
    "    :return: 在指定标签内的内容\n",
    "    \"\"\"\n",
    "    pattern = f'^.*<{answer_label}>(.+)</{answer_label}>.*$'\n",
    "    match=re.search(pattern,completion,re.DOTALL)\n",
    "    if match:\n",
    "        answer=match.group(1)\n",
    "    else:\n",
    "        answer=None\n",
    "    return answer\n",
    "\n",
    "# 内容奖励\n",
    "def reward_content(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    补全的内容比回答的内容长就奖励\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for idx,completion in enumerate(completions):\n",
    "\n",
    "        response_answer = extract_answer(completion[0]['content'])\n",
    "        if response_answer is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "\n",
    "        dlen = len(answer[idx]) - len(response_answer)\n",
    "        if dlen > 0:\n",
    "            scores.append(5)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "# 宽松标签奖励\n",
    "def reward_label(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    只要有 <answer> 标签就奖励\n",
    "    \"\"\"\n",
    "    print(completions)\n",
    "\n",
    "    pattern = f'^.*<{answer_label}>(.+)</{answer_label}>.*$'\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if re.fullmatch(pattern, completion[0]['content']):\n",
    "            scores.append(5)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "# 严格标签奖励\n",
    "def reward_label_strict(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    必须按照 <answer> ... </answer> 的格式输出，才有奖励\n",
    "    \"\"\"\n",
    "    pattern = f'^<{answer_label}>(.+)</{answer_label}>$'\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if re.fullmatch(pattern, completion[0]['content']):\n",
    "            scores.append(10)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores"
   ],
   "id": "94a7063030eef8f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. 进行训练\n",
    "\n",
    "GRPO 训练器配置参数 `GRPOConfig` 继承自 `TrainingArguments` ，下面是其相关配置：：\n",
    "\n",
    "| 分类            | 参数名称                          | 类型/选项                     | 默认值      | 说明                                                               |\n",
    "|---------------|-------------------------------|---------------------------|----------|------------------------------------------------------------------|\n",
    "| **模型和参考模型**   | `model_init_kwargs`           | `dict[str, Any]` 或 `None` | `None`   | 用于 [`~transformers.AutoModelForCausalLM.from_pretrained`] 的关键字参数 |\n",
    "| **数据预处理**     | `remove_unused_columns`       | `bool`                    | `False`  | 是否仅保留数据集中的 `\"prompt\"` 列                                          |\n",
    "|               | `max_prompt_length`           | `int` 或 `None`            | `512`    | 提示词最大长度（超长时左截断）                                                  |\n",
    "|               | `num_generations`             | `int` 或 `None`            | `8`      | 每个提示词的生成次数（全局批次大小必须可被此值整除）                                       |\n",
    "|               | `temperature`                 | `float`                   | `0.9`    | 采样温度值（越高随机性越强）                                                   |\n",
    "|               | `max_completion_length`       | `int` 或 `None`            | `256`    | 生成结果的最大长度                                                        |\n",
    "|               | `ds3_gather_for_generation`   | `bool`                    | `True`   | DeepSpeed ZeRO-3 下是否聚合权重加速生成（禁用可训练超显存模型但速度降低）                    |\n",
    "| **vLLM 生成加速** | `use_vllm`                    | `bool`                    | `False`  | 是否使用 vLLM 加速生成（需单独保留 GPU）                                        |\n",
    "|               | `vllm_device`                 | `str`                     | `\"auto\"` | vLLM 生成设备（自动选择可用 GPU）                                            |\n",
    "|               | `vllm_gpu_memory_utilization` | `float`                   | `0.9`    | vLLM GPU 内存利用率（高值提升吞吐但可能 OOM）                                    |\n",
    "|               | `vllm_dtype`                  | `str`                     | `\"auto\"` | vLLM 生成数据类型（自动根据模型配置选择）                                          |\n",
    "|               | `vllm_max_model_len`          | `int` 或 `None`            | `None`   | vLLM 最大模型长度（优化显存不足时的 KV 缓存效率）                                    |\n",
    "| **训练控制**      | `learning_rate`               | `float`                   | `1e-6`   | AdamW 优化器的初始学习率（覆盖父类默认值）                                         |\n",
    "|               | `beta`                        | `float`                   | `0.04`   | KL 散度系数                                                          |\n",
    "|               | `reward_weights`              | `list[float]` 或 `None`    | `None`   | 奖励函数权重列表（未设置时等权 1.0）                                             |\n",
    "|               | `sync_ref_model`              | `bool`                    | `False`  | 是否定期同步参考模型（需配合 TR-DPO 参数）                                        |\n",
    "|               | `ref_model_mixup_alpha`       | `float`                   | `0.9`    | TR-DPO 混合系数（π_ref = απ_θ + (1-α)π_ref_prev）                      |\n",
    "|               | `ref_model_sync_steps`        | `int`                     | `64`     | TR-DPO 参考模型同步频率                                                  |\n",
    "| **日志控制**      | `log_completions`             | `bool`                    | `False`  | 是否在训练期间记录生成结果                                                    |\n",
    "\n",
    "注意，推荐使用 vllm 进行加速，即令 `use_vllm = True` ，但如果在 windows 系统下直接安装 vllm 可能会出现一些问题，请参考下面方案进行安装 ：[vllm在Windows系统下的安装解决方案](docs/vllm在Windows系统下的安装解决方案.md)"
   ],
   "id": "3221c0bf9051c087"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"E:/AI/Models/Qwen/Qwen2.5-0.5B\"\n",
    "output_dir = \"./output/grpo\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir = output_dir,\n",
    "\n",
    "    num_train_epochs = 1,            # 训练轮数\n",
    "    learning_rate = 5e-6,            # 学习率\n",
    "    lr_scheduler_type = \"cosine\",    # 学习率调度器的类型，这里采用余弦函数衰减\n",
    "    adam_beta1 = 0.9,                # AdamW 一阶矩估计的衰减率 beta1\n",
    "    adam_beta2 = 0.99,               # AdamW 二阶矩估计的衰减率 beta2\n",
    "    weight_decay = 0.1,              # 权重衰减（L2 正则化），防止模型过拟合，通过在损失函数中添加权重的 L2 范数来约束模型参数。\n",
    "\n",
    "    per_device_train_batch_size = 2, # 每个设备训练的批大小\n",
    "    gradient_accumulation_steps = 4, # 累积梯度，等效 batch_size=2*4=8\n",
    "\n",
    "    bf16 = True,                     # 使用混合精度\n",
    "\n",
    "    num_generations = 2,             # 每个提示词生成的次数\n",
    "    max_prompt_length = 256,         # 最大提示词长度\n",
    "    max_completion_length = 300,     # 最大补全长度\n",
    "    save_steps = 100,\n",
    "    logging_steps=10,\n",
    "    max_grad_norm = 0.1,             # 梯度裁剪的最大范数，防止梯度爆炸，通过裁剪梯度的范数使其不超过设定值。\n",
    "    warmup_ratio = 0.1,              # 学习率预热的比例。\n",
    "    # report_to = \"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_path,\n",
    "    reward_funcs=[\n",
    "        reward_label,\n",
    "        reward_label_strict,\n",
    "        reward_content\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "c865446c7de18b5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. 保存模型",
   "id": "75546671961abb52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 保存路径\n",
    "output_dir = \"./output/final\"\n",
    "\n",
    "trainer.model.save_pretrained(output_dir)"
   ],
   "id": "dee191cd68606278",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
