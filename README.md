# 大语言模型微调

大语言模型微调是指在预训练的大规模语言模型（如GPT、BERT等）基础上，针对特定任务或领域数据进行进一步训练，使其适应具体应用场景的技术。预训练模型通过海量通用数据学习了通用的语言模式和知识，而微调则利用特定数据集（如医疗文本、客服对话等），调整模型参数以优化其在目标任务（如文本分类、问答系统）上的表现。这种方法既保留了模型的通用能力，又通过针对性训练提升了专业领域的准确性和适应性，显著降低了从头训练的成本，成为高效适配行业需求的关键手段。

## 微调实践

在本次实践中我们将介绍两种典型的模型微调方案：

1. 基于 LoRA 的监督微调： [SFT-LoRA](1_SFT-LoRA.ipynb)
2. 基于 GRPO 的强化学习： [RL-GRPO](2_RL-GRPO.ipynb)