{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ä½¿ç”¨ LoRA å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ\n",
    "\n",
    "## LoRA åŸç†\n",
    "\n",
    "**LoRAï¼ˆLow-Rank Adaptationï¼Œä½ç§©è‡ªé€‚åº”ï¼‰** æ˜¯ä¸€ç§é«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä½ç§©åˆ†è§£ï¼Œåœ¨åŸå§‹æ¨¡å‹å‚æ•°æ—æ·»åŠ å°‘é‡å¯è®­ç»ƒçš„â€œä½ç§©çŸ©é˜µâ€æ¥æ¨¡æ‹Ÿå‚æ•°æ›´æ–°ï¼Œè€Œéç›´æ¥è°ƒæ•´åºå¤§çš„åŸå§‹å‚æ•°ã€‚è¿™ç§æ–¹æ³•å¤§å¹…å‡å°‘äº†è®­ç»ƒå‚æ•°é‡ï¼ˆé€šå¸¸é™ä½ä¸‡å€ä»¥ä¸Šï¼‰ï¼Œæ˜¾è‘—èŠ‚çœè®¡ç®—èµ„æºå’Œå­˜å‚¨å¼€é”€ï¼ŒåŒæ—¶ä¿æŒä¸å…¨å‚æ•°å¾®è°ƒç›¸è¿‘çš„æ•ˆæœï¼Œä¸”æ¨ç†æ—¶å¯é€šè¿‡åˆå¹¶çŸ©é˜µé¿å…é¢å¤–å»¶è¿Ÿã€‚LoRAç‰¹åˆ«é€‚åˆèµ„æºæœ‰é™æˆ–éœ€è¦å¿«é€Ÿé€‚é…å¤šä»»åŠ¡çš„å¤§æ¨¡å‹åœºæ™¯ã€‚å…¶åŸç†å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š\n",
    "\n",
    "![LoRA Struct](img/LoRA-Struct.svg)\n",
    "\n",
    "ï¼ˆå›¾ä¸­å·¦ä¾§è¡¨ç¤ºå…¨å‚æ•°å¾®è°ƒï¼Œå³ä¾§è¡¨ç¤º LoRA å¾®è°ƒï¼Œè“è‰²éƒ¨åˆ†è¡¨ç¤ºéœ€è¦è®­ç»ƒçš„æƒé‡ï¼Œç°è‰²éƒ¨åˆ†è¡¨ç¤ºå†»ç»“çš„æƒé‡ä¸å‚ä¸è®­ç»ƒï¼‰\n",
    "\n",
    "ç»è¿‡ LoRA å¾®è°ƒåçš„æ¨¡å‹ç»“æœä¼šäº§ç”Ÿä¸€ä¸ªé¢å¤–çš„ LoRA æƒé‡å¯¹åº”äºå›¾ä¸­çš„è™šçº¿æ¡†éƒ¨åˆ†ï¼Œä¸‹æ¬¡è°ƒç”¨æ¨¡å‹æ—¶å°† LoRA æƒé‡åŠ è½½åˆ°åŸºç¡€æ¨¡å‹ä¸Šå³å¯ã€‚\n",
    "\n",
    "## LoRA è®­ç»ƒ\n",
    "\n",
    "åœ¨å¼€æºæ¨¡å‹æŠ€æœ¯è“¬å‹ƒå‘å±•çš„å½“ä¸‹ï¼Œä»¥ LoRA ä¸ºä»£è¡¨çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å·²æˆä¸ºå¤§æ¨¡å‹é€‚é…ä¸‹æ¸¸ä»»åŠ¡çš„é‡è¦èŒƒå¼ã€‚å°½ç®¡å½“å‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹çš„æ¶æ„å‚æ•°å¤§å¤šå·²å¼€æºï¼Œ**ä½†å·¥ç¨‹å®è·µä¸­å¼€å‘è€…æ›´å…³æ³¨å¾®è°ƒç­–ç•¥çš„å®æ–½è€Œéåº•å±‚ç½‘ç»œç»“æ„çš„å®ç°ç»†èŠ‚**ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡èšç„¦å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒè®­ç»ƒå®è·µï¼ŒåŸºäºHugging Faceç”Ÿæ€ä¸­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåº“TRLï¼ˆTransformer Reinforcement Learningï¼‰ï¼Œç³»ç»Ÿè§£æç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰ç­‰æ ¸å¿ƒæŠ€æœ¯çš„å®ç°è·¯å¾„ã€‚\n",
    "\n",
    "TRLä½œä¸ºğŸ¤— Transformersç”Ÿæ€çš„é‡è¦ç»„ä»¶ï¼Œé›†æˆäº†ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰ã€è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰å‰æ²¿è®­ç»ƒèŒƒå¼ã€‚è¯¥åº“ä¸ä»…æ”¯æŒå¤šæ¨¡æ€æ¨¡å‹çš„çµæ´»é€‚é…ï¼Œæ›´èƒ½é€šè¿‡åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶å®ç°ä»å•å¡åˆ°é›†ç¾¤çš„å¼¹æ€§æ‰©å±•ï¼Œä¸ºä¸åŒç¡¬ä»¶ç¯å¢ƒä¸‹çš„æ¨¡å‹è°ƒä¼˜æä¾›ç»Ÿä¸€çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚å…¶æ¨¡å—åŒ–è®¾è®¡å…¼é¡¾ç®—æ³•åˆ›æ–°ä¸å·¥ç¨‹è½åœ°ï¼Œå¼€å‘è€…æ—¢å¯ç›´æ¥è°ƒç”¨é¢„ç½®è®­ç»ƒæµç¨‹ï¼Œä¹Ÿå¯åŸºäºAPIçµæ´»æ„å»ºå®šåˆ¶åŒ–è®­ç»ƒç­–ç•¥ã€‚\n",
    "\n",
    "- TRL å¼€æºï¼š[Github - trl](https://github.com/huggingface/trl)\n",
    "- TRL æ–‡æ¡£ï¼š[huggingface - trl](https://hf.co/docs/trl)\n",
    "\n",
    "### 1. å¼•å…¥å¿…è¦çš„åº“"
   ],
   "id": "34d7a4fa068ac2cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model\n",
    ")\n",
    "from trl import SFTConfig,SFTTrainer\n",
    "from datasets import load_dataset, Dataset"
   ],
   "id": "a9a07c08359c4d26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. å‡†å¤‡æ•°æ®\n",
    "\n",
    "æœ¬åœ°å·²ç»ä¸‹è½½å¥½äº†ç”¨äºè®­ç»ƒçš„æ•°æ®å­˜æ”¾åœ¨ `.\\dataset\\Capybara` ç›®å½•ä¸‹ã€‚"
   ],
   "id": "1cbe15fd2ff25b97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# å¯ä»¥ç›´æ¥ä» huggingface çš„åº“ä¸­è¿›è¡Œä¸‹è½½\n",
    "# dataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n",
    "\n",
    "dataset = Dataset.load_from_disk('./dataset/Capybara')\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['messages'])):\n",
    "        text = f\"### Question: {example['messages'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ],
   "id": "918a019412c85b96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. ä½¿ç”¨ LoRA çš„æ–¹å¼åŠ è½½æ¨¡å‹\n",
    "\n",
    "ä¸‹é¢æ˜¯ `LoraConfig` çš„é…ç½®å‚æ•°ï¼š\n",
    "\n",
    "| åˆ†ç±»                  | å‚æ•°åç§°                    | ç±»å‹/é€‰é¡¹                                                                 | è¯´æ˜                                                                                                                                                     | å¤‡æ³¨                                                                                     |\n",
    "|-----------------------|----------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
    "| **åŸºç¡€é…ç½®**          | `r`                        | `int`                                                                   | LoRA çš„ç§©ï¼ˆRankï¼‰ï¼Œå†³å®šä½ç§©çŸ©é˜µçš„ç»´åº¦                                                                                                                     | å¸¸ç”¨èŒƒå›´ 8-64                                                                           |\n",
    "|                       | `lora_alpha`               | `int`                                                                   | LoRA çš„ç¼©æ”¾ç³»æ•°ï¼Œä¸ `r` å…±åŒæ§åˆ¶ä½ç§©çŸ©é˜µçš„ç¼©æ”¾å¹…åº¦                                                                                                         | é»˜è®¤ç¼©æ”¾æ¯”ä¾‹ï¼š`lora_alpha/r`ï¼ˆæˆ– `lora_alpha/sqrt(r)` è‹¥å¯ç”¨ `use_rslora`ï¼‰               |\n",
    "|                       | `lora_dropout`             | `float`                                                                 | LoRA å±‚çš„ Dropout æ¦‚ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ                                                                                                                       | èŒƒå›´ 0.0-1.0ï¼Œå¸¸ç”¨ 0.1-0.3                                                              |\n",
    "|                       | **`task_type`**            | `\"CAUSAL_LM\"` / `\"SEQ_CLS\"` / `\"SEQ_2_SEQ_LM\"` / `\"TOKEN_CLS\"` ç­‰        | **æŒ‡å®šæ¨¡å‹çš„ä»»åŠ¡ç±»å‹**ï¼Œå†³å®šé€‚é…å™¨çš„æ’å…¥ä½ç½®å’Œè®­ç»ƒè¡Œä¸º                                                                                                     | å¿…å¡«é¡¹ï¼Œéœ€ä¸æ¨¡å‹æ¶æ„åŒ¹é…ï¼ˆå¦‚ `\"CAUSAL_LM\"` å¯¹åº” GPTã€Llama ç­‰è‡ªå›å½’æ¨¡å‹ï¼‰                 |\n",
    "| **ç›®æ ‡æ¨¡å—é€‰æ‹©**      | `target_modules`           | `List[str]` / `str` / `\"all-linear\"`                                    | æŒ‡å®šåº”ç”¨ LoRA çš„æ¨¡å—åç§°ï¼ˆæ”¯æŒæ­£åˆ™åŒ¹é…ã€åç¼€åŒ¹é…æˆ–è‡ªåŠ¨é€‰æ‹©æ‰€æœ‰çº¿æ€§å±‚ï¼‰                                                                                     | ç¤ºä¾‹ï¼š`[\"query\", \"value\"]`                                                               |\n",
    "|                       | `exclude_modules`          | `List[str]` / `str`                                                     | æ’é™¤ä¸éœ€è¦åº”ç”¨ LoRA çš„æ¨¡å—                                                                                                                               | ä¼˜å…ˆçº§é«˜äº `target_modules`                                                              |\n",
    "|                       | `layers_to_transform`      | `List[int]` / `int`                                                     | æŒ‡å®šè¦è½¬æ¢çš„å±‚ç´¢å¼•                                                                                                                                       | ç¤ºä¾‹ï¼š`[0, 1]` è¡¨ç¤ºä»…ä¿®æ”¹å‰ä¸¤å±‚                                                           |\n",
    "|                       | `layers_pattern`           | `List[str]` / `str`                                                     | æ¨¡å‹å±‚ç»“æ„çš„åç§°æ¨¡å¼ï¼ˆå¦‚ `\"h\"` å¯¹åº” GPT-2 çš„å±‚åˆ—è¡¨ï¼‰                                                                                                      |                                                                                          |\n",
    "| **é«˜çº§åˆå§‹åŒ–ä¸ä¼˜åŒ–**  | `init_lora_weights`        | `bool` / `\"gaussian\"` / `\"eva\"` / `\"pissa\"` / `\"corda\"` / `\"loftq\"` ç­‰   | æ§åˆ¶ LoRA æƒé‡åˆå§‹åŒ–æ–¹å¼ï¼š<br>- `True`ï¼šé»˜è®¤åˆå§‹åŒ–ï¼ˆB çŸ©é˜µä¸º 0ï¼‰<br>- `\"eva\"`ï¼šåŸºäºæ•°æ® SVD çš„ä¼˜åŒ–åˆå§‹åŒ–<br>- `\"pissa\"`ï¼šå¿«é€Ÿ SVD åˆå§‹åŒ–åŠ é€Ÿæ”¶æ•›            | éœ€é…åˆ `eva_config`/`loftq_config` ç­‰ä½¿ç”¨                                                |\n",
    "|                       | `use_rslora`               | `bool`                                                                  | å¯ç”¨ Rank-Stabilized LoRAï¼Œè°ƒæ•´ç¼©æ”¾å…¬å¼ä¸º `lora_alpha/sqrt(r)`                                                                                           |                                                                                          |\n",
    "|                       | `use_dora`                 | `bool`                                                                  | å¯ç”¨ DoRAï¼Œåˆ†è§£æƒé‡æ›´æ–°ä¸ºå¹…åº¦å’Œæ–¹å‘ï¼ˆæå‡ä½ç§©æ€§èƒ½ï¼Œä½†å¢åŠ è®¡ç®—å¼€é”€ï¼‰                                                                                       |                                                                                          |\n",
    "| **ç‰¹å®šåœºæ™¯é…ç½®**      | `bias`                     | `\"none\"` / `\"lora_only\"` / `\"all\"`                                      | æ§åˆ¶æ˜¯å¦æ›´æ–°åç½®é¡¹                                                                                                                                       | `\"all\"` æˆ– `\"lora_only\"` æ—¶éœ€æ³¨æ„ç¦ç”¨é€‚é…å™¨åè¾“å‡ºå¯èƒ½ä¸åŸæ¨¡å‹ä¸åŒ                          |\n",
    "|                       | `fan_in_fan_out`           | `bool`                                                                  | è‹¥æ¨¡å‹æƒé‡å½¢çŠ¶ä¸º `(fan_in, fan_out)`ï¼ˆå¦‚ GPT-2 çš„ `Conv1D`ï¼‰ï¼Œéœ€è®¾ä¸º `True`                                                                              |                                                                                          |\n",
    "|                       | `megatron_config`          | `dict`                                                                  | ç”¨äº Megatron æ¡†æ¶çš„å¹¶è¡Œçº¿æ€§å±‚é…ç½®                                                                                                                       | éœ€é…åˆ `megatron_core=\"megatron.core\"` ä½¿ç”¨                                               |\n",
    "|                       | `trainable_token_indices`  | `List[int]` / `dict`                                                    | ä»…å¾®è°ƒåµŒå…¥å±‚çš„ç‰¹å®š tokenï¼ˆå¦‚ç‰¹æ®Šæ ‡è®°ï¼‰                                                                                                                   | ç¤ºä¾‹ï¼š`{'embed_tokens': [0, 1]}`                                                         |\n",
    "| **æ¨¡å¼æ‰©å±•ä¸è‡ªå®šä¹‰**  | `rank_pattern`             | `dict`                                                                  | ä¸ºä¸åŒå±‚æŒ‡å®šç‹¬ç«‹çš„ç§©ï¼ˆ`r`ï¼‰                                                                                                                              | ç¤ºä¾‹ï¼š`{\"attention.*\": 16}` å¯¹æ³¨æ„åŠ›å±‚ä½¿ç”¨ç§© 16                                           |\n",
    "|                       | `alpha_pattern`            | `dict`                                                                  | ä¸ºä¸åŒå±‚æŒ‡å®šç‹¬ç«‹çš„ç¼©æ”¾ç³»æ•°ï¼ˆ`alpha`ï¼‰                                                                                                                    |                                                                                          |\n",
    "|                       | `layer_replication`        | `List[Tuple[int, int]]`                                                 | å¤åˆ¶åŸå§‹å±‚æ„å»ºæ–°å±‚ï¼ˆæ‰©å±•æ¨¡å‹æ·±åº¦ï¼‰ï¼Œæ¯ä¸ªæ–°å±‚ç‹¬ç«‹é€‚é…å™¨                                                                                                   | ç¤ºä¾‹ï¼š`[(0, 3)]` è¡¨ç¤ºå¤åˆ¶ç¬¬ 0-3 å±‚                                                        |\n",
    "| **å…¶ä»–**              | `modules_to_save`          | `List[str]`                                                             | é¢å¤–è®­ç»ƒå¹¶ä¿å­˜çš„æ¨¡å—ï¼ˆå¦‚åˆ†ç±»å¤´ï¼‰                                                                                                                         | ç¤ºä¾‹ï¼š`[\"classifier\"]`                                                                   |\n",
    "|                       | `lora_bias`                | `bool`                                                                  | æ˜¯å¦å¯ç”¨ LoRA B çŸ©é˜µçš„åç½®ï¼ˆé»˜è®¤å…³é—­ï¼‰                                                                                                                   |                                                                                          |"
   ],
   "id": "7a8bf22927520951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LoRA é…ç½®\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # LoRA æ³¨å…¥çš„æ¨¡å—\n",
    "    task_type=TaskType.CAUSAL_LM,               # å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä½¿ç”¨ CAUSAL_LM\n",
    ")\n",
    "\n",
    "# é‡åŒ–é…ç½®ï¼ˆå‡å°æ¨¡å‹åŠ è½½å’Œè®­ç»ƒæ—¶çš„æ˜¾å­˜ï¼Œä½†ç²¾åº¦ä¼šä¸‹é™ï¼‰\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,                    # å°†æ¨¡å‹é‡åŒ–ä¸º 8bit è¿›è¡ŒåŠ è½½\n",
    "    # load_in_4bit=True,                  # å°†æ¨¡å‹é‡åŒ–ä¸º 4bit è¿›è¡ŒåŠ è½½\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16,\n",
    "    # bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹ï¼Œå¯ä»¥ä¸ºæ¨¡å‹è·¯å¾„æˆ–è€…æ¨¡å‹ID\n",
    "# model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_id = \"E:/Model/Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True  # ç¡®ä¿Qwenæ¨¡å‹å…¼å®¹\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# æ‰“å°æ¨¡å‹ç»“æ„å’Œé¢„æœŸå¯ä»¥è®­ç»ƒçš„å‚æ•°\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "75a224b77db0207d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. è¿›è¡Œè®­ç»ƒ\n",
    "\n",
    "å¾®è°ƒé…ç½®å‚æ•° `SFTConfig` ç»§æ‰¿è‡ª `TrainingArguments` ï¼Œä¸‹é¢æ˜¯å…¶ç›¸å…³é…ç½®ï¼š\n",
    "\n",
    "| åˆ†ç±»                     | å‚æ•°åç§°                | ç±»å‹/é»˜è®¤å€¼                                                                 | è¯´æ˜                                                                                           |\n",
    "|--------------------------|-------------------------|----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **æ¨¡å‹æ§åˆ¶å‚æ•°**         | `model_init_kwargs`     | `dict[str, Any]` / `None`                                                 | ç”¨äº `AutoModelForCausalLM.from_pretrained` çš„å…³é”®å­—å‚æ•°ï¼ˆå½“æ¨¡å‹ä»¥å­—ç¬¦ä¸²å½¢å¼ä¼ å…¥æ—¶ï¼‰             |\n",
    "|                          | `use_liger`             | `bool` / `False`                                                          | æ˜¯å¦ä½¿ç”¨ Liger å†…æ ¸ä¼˜åŒ–æ¨¡å‹ï¼ˆæå‡ååé‡å¹¶å‡å°‘å†…å­˜å ç”¨ï¼‰                                          |\n",
    "| **æ•°æ®é¢„å¤„ç†å‚æ•°**       | `dataset_text_field`    | `str` / `\"text\"`                                                          | æ•°æ®é›†ä¸­åŒ…å«æ–‡æœ¬æ•°æ®çš„åˆ—å                                                                       |\n",
    "|                          | `dataset_kwargs`        | `dict[str, Any]` / `None`                                                 | æ•°æ®é›†é¢„å¤„ç†çš„å¯é€‰å‚æ•°ï¼ˆä»…æ”¯æŒ `skip_prepare_dataset` é”®ï¼‰                                       |\n",
    "|                          | `dataset_num_proc`      | `int` / `None`                                                            | æ•°æ®é›†é¢„å¤„ç†ä½¿ç”¨çš„è¿›ç¨‹æ•°                                                                         |\n",
    "|                          | `max_seq_length`        | `int` / `1024`                                                            | åºåˆ—æœ€å¤§é•¿åº¦ï¼ˆè¶…é•¿éƒ¨åˆ†æˆªæ–­å³ç«¯ï¼‰ï¼Œè®¾ä¸º `None` åˆ™ä¸æˆªæ–­                                           |\n",
    "|                          | `packing`               | `bool` / `False`                                                          | æ˜¯å¦å°†å¤šåºåˆ—æ‰“åŒ…ä¸ºå›ºå®šé•¿åº¦æ ¼å¼ï¼ˆä½¿ç”¨ `max_seq_length` å®šä¹‰é•¿åº¦ï¼‰                                 |\n",
    "|                          | `eval_packing`          | `bool` / `None`                                                           | è¯„ä¼°é›†æ˜¯å¦æ‰“åŒ…ï¼ˆ`None` æ—¶ä¸ `packing` å€¼ç›¸åŒï¼‰                                                   |\n",
    "| **è®­ç»ƒæ§åˆ¶å‚æ•°**         | `learning_rate`         | `float` / `2e-5`                                                          | AdamW ä¼˜åŒ–å™¨çš„åˆå§‹å­¦ä¹ ç‡ï¼ˆè¦†ç›– `TrainingArguments` é»˜è®¤å€¼ï¼‰                                      |"
   ],
   "id": "63b2deccb92f328f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./output/lora\",\n",
    "\n",
    "    num_train_epochs=3,              # è®­ç»ƒè½®æ•°\n",
    "    learning_rate=2e-4,              # å­¦ä¹ ç‡\n",
    "\n",
    "    per_device_train_batch_size=2,   # æ¯ä¸ªè®¾å¤‡è®­ç»ƒçš„æ‰¹å¤§å°\n",
    "    gradient_accumulation_steps=8,   # ç´¯ç§¯æ¢¯åº¦ï¼Œç­‰æ•ˆ batch_size=2*8=16\n",
    "    gradient_checkpointing=True,     # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²20%é€Ÿåº¦æ¢æ˜¾å­˜ï¼‰\n",
    "\n",
    "    fp16=True,                       # ä½¿ç”¨æ··åˆç²¾åº¦\n",
    "    # torch_compile=False,           # ç¦ç”¨æ¨¡å‹ç¼–è¯‘ï¼ˆå‡å°‘åˆå§‹æ˜¾å­˜å³°å€¼ï¼‰\n",
    "\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    optim=\"adamw_torch\",             # ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨\n",
    "    warmup_ratio=0.1,                # å‰ 10% æ­¥æ•°åšå­¦ä¹ ç‡é¢„çƒ­\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model= model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "3570897229965f90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.ä¿å­˜ LoRA æ¨¡å‹",
   "id": "3a3845cc441b5229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ä¿å­˜è·¯å¾„\n",
    "output_dir = \"./output/sft/lora\"\n",
    "\n",
    "# åªä¿å­˜ LoRA é€‚é…å™¨ï¼ˆçº¦å‡ åMBï¼‰\n",
    "model.save_pretrained(output_dir)  # ç”Ÿæˆ adapter_config.json + adapter_model.safetensors"
   ],
   "id": "6edf3d49aa330b45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LoRA æ¨¡å‹åˆå¹¶",
   "id": "ed48444013a49d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(\"./output/sft/lora_merged_model\")"
   ],
   "id": "a6c0c69876df1426",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
